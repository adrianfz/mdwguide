{"cells":[{"cell_type":"markdown","source":["###Lab 2 Semi-structured Data\n\nThis notebook to perform data wrangling on Clickstream data \n\nLeverages Spark Data Frames Library\n\nClickstream data from https://2015.recsyschallenge.com/"],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Set the data location and type\n\nThere are two ways to access Azure Blob storage: account keys and shared access signatures (SAS).\n\nTo get started, we need to set the location and type of the file.\n\n\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse"],"metadata":{}},{"cell_type":"code","source":["storage_account_name = \"<Storage Account Name Here>\"\nstorage_account_access_key = \"<Storage Account Key Here>\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["file_location = \"wasbs://<storage_account_name>.blob.core.windows.net/yoochoose-clicks.dat\"\nfile_type = \"csv\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["spark.conf.set(\n  \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n  storage_account_access_key)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(dbutils.fs.ls(\"wasbs://<container_name>@<storage_account_name>.blob.core.windows.net/\"))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Step 2: Read the data\n\nNow that we have specified our file metadata, we can create a DataFrame. Notice that we use an *option* to specify that we want to infer the schema from the file. We can also explicitly set this to a particular schema if we have one already.\n\n####Best Python Libraries for Data Transformations\n\nPython (which libraries -- DF or Pandas)\n\npetl\n\npandas\n\nSQLAlchemy\n\nBonobo\n\nhttps://github.com/pawl/awesome-etl#python\n\nhttps://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html (Arrow to convert Spark to PD)"],"metadata":{}},{"cell_type":"markdown","source":["####Transformation links w/ Spark DataFrames\n\nhttps://mapr.com/blog/using-apache-spark-dataframes-processing-tabular-data/\n\nhttps://mapr.com/products/apache-spark/\n\nhttps://docs.azuredatabricks.net/_static/notebooks/transform-complex-data-types-python.html \n\nhttps://thepythonguru.com/python-how-to-read-and-write-csv-files/\n\nhttps://docs.azuredatabricks.net/data/data-sources/read-csv.html"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(file_type).option(\"inferSchema\", \"true\").load(file_location)\ndf = df.withColumnRenamed(\"_c0\", \"SessionID\")\ndf = df.withColumnRenamed(\"_c1\", \"Timestamp\")\ndf = df.withColumnRenamed(\"_c2\", \"ProductID\")\ndf = df.withColumnRenamed(\"_c3\", \"Category\")\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(df.summary())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Step 3: Query the data\n\nNow that we have created our DataFrame, we can query it. For instance, you can identify particular columns to select and display.\n\n#### Links to Query data\nhttps://spark.apache.org/docs/1.6.0/sql-programming-guide.html\n\nhttps://hackersandslackers.com/transforming-pyspark-dataframes/"],"metadata":{}},{"cell_type":"code","source":["display(df.select(\"SessionID\",\"Timestamp\",\"ProductID\",\"Category\"))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Step 4: (Optional) Create a view or table\n\nIf you want to query this data as a table, you can simply register it as a *view* or a table."],"metadata":{}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"Clicks\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["We can query this view using Spark SQL. For instance, we can perform a simple aggregation. Notice how we can use `%sql` to query the view from SQL."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM Clicks"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Since this table is registered as a temp view, it will be available only to this notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame."],"metadata":{}},{"cell_type":"code","source":["df.write.format(\"parquet\").saveAsTable(\"Clicks\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\n\nSELECT \nSessionID,\nmin(Timestamp) as FirstTimestamp, \nmax(Timestamp) as LastTimestamp,\nunix_timestamp(max(Timestamp)) - unix_timestamp(min(Timestamp)) as Duration,\nmax(Category) as LastCategory,\nmin(ProductID) as FirstProduct, \nmax(ProductID) as LastProduct,\nCOUNT(*) as SessionCount \nFROM Clicks \ngroup by SessionID \norder by SessionID"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndfsum=sqlContext.sql(\"SELECT SessionID,min(Timestamp) as FirstTimestamp, max(Timestamp) as LastTimestamp,unix_timestamp(max(Timestamp)) - unix_timestamp(min(Timestamp)) as Duration,max(Category) as LastCategory,min(ProductID) as FirstProduct, max(ProductID) as LastProduct, Count(*) as SessionCount FROM Clicks group by SessionID order by SessionID\")\ndfsum.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dfsum.write.partitionBy(\"LastCategory\").format(\"parquet\").saveAsTable(\"Clicks_Summary\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["This table will persist across cluster restarts and allow various users across different notebooks to query this data.\n\nhttps://docs.azuredatabricks.net/data/databricks-file-system.html"],"metadata":{}},{"cell_type":"code","source":["%python\ndisplay(dbutils.fs.ls(\"/user/hive/warehouse/clicks_summary/\"))"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Clickstream_ETL","notebookId":928039200218530},"nbformat":4,"nbformat_minor":0}
